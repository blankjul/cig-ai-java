\section{Conclusions and Future Work} 
\label{sec:conc}

This paper presents a comparison of several approaches concerning the playing of general video games. For each research area (\ac{HR}, \ac{RL}, \ac{NI})
we selected one algorithm that we implemented.


For the evaluation we have to look at the data such as average wins, score and time steps.
We ignored the in-game behaviour of our agents completely and only considered the statistics. 
Furthermore we were limited by time and computational power which restricts the iterations of our experiment. 
In order to produce more significant results, a experimental setup with more than 1000 iterations (like 3000 in the comparison between the best agents) has to be set up.
Some future work could be to set this setup up and to test the algorithms with more computing power.  
The \ac{HR} approach achieved the best results (depending on the number of wins) and proved to be better than the \ac{NI} and \ac{MCTS}. This might have several reasons. Firstly our implementation of \ac{MCTS} and \ac{NI} is might not be very effective or we might have failed to find the optimal parameters to fit our problem. The parameter problem, which occurs due to the lack of computing power, could be solved with a more powerful machine and a loop over a set of possible parameter-combinations.
Another reason why the \ac{HR} agent wins most of the games is that often there are only very few winning-sprites in the game which makes it hard for \ac{MCTS} and \ac{NI} to get a reward and win the game. These games are easy for the heuristic controller. The average score of the \ac{MCTS} controller is higher because the \ac{HR} controller wins very fast without collecting extra points. 
We cannot make general statements about the accuracy of the approaches (\acs{HR}, \acs{RL}, \acs{NI}) using our controllers as quality criterion, but we can say that all of them are useful for \ac{GVGP}.
For future tasks, it would be interesting to compare the implementation of other controllers and methods (Neuronal Nets, Pheromone) to the actual ones. Furthermore a test with unknown games would be interesting, to compare the agents and the methods on completely unknown games.
Another idea for future work is to combine the best properties of the three agents and and create a combined agent.
